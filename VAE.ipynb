{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10226ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea09320",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder (VAE) for image reconstruction.\n",
    "    Includes encoder, reparameterization trick, and decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=128, img_channels=3):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder: Downsampling the image into a latent vector\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, 32, 4, 2, 1),  # Output: [B, 32, 128, 128]\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1),            # Output: [B, 64, 64, 64]\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),           # Output: [B, 128, 32, 32]\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),          # Output: [B, 256, 16, 16]\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),          # Output: [B, 512, 8, 8]\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(512, 512, 4, 2, 1),          # Output: [B, 512, 4, 4]\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        # Latent space\n",
    "        self.fc_mu = nn.Linear(512 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(512 * 4 * 4, latent_dim)\n",
    "\n",
    "        # Decoder: Upsampling the latent vector to image\n",
    "        self.fc_decode = nn.Linear(latent_dim, 512 * 4 * 4)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 512, 4, 2, 1),  # [B, 512, 8, 8]\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1),  # [B, 256, 16, 16]\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),  # [B, 128, 32, 32]\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),   # [B, 64, 64, 64]\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),    # [B, 32, 128, 128]\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(32, 3, 4, 2, 1),     # [B, 3, 256, 256]\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        print(f\"Input to encoder: {x.shape}\")\n",
    "        x = self.encoder(x)\n",
    "        print(f\"Output of encoder conv layers: {x.shape}\")\n",
    "        x = x.view(x.size(0), -1)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        print(f\"Latent mean shape: {mu.shape}, logvar shape: {logvar.shape}\")\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        print(f\"Sampled latent vector z shape: {z.shape}\")\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        x = self.fc_decode(z)\n",
    "        x = x.view(x.size(0), 512, 4, 4)\n",
    "        print(f\"Input to decoder reshape: {x.shape}\")\n",
    "        x = self.decoder(x)\n",
    "        print(f\"Output of decoder: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z)\n",
    "        return recon, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acd783c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x, recon, mu, logvar):\n",
    "    \"\"\"\n",
    "    VAE Loss = Reconstruction Loss + Î² * KL Divergence\n",
    "    \"\"\"\n",
    "    beta = 0.1\n",
    "    recon_loss = F.mse_loss(recon, x, reduction='mean')\n",
    "    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n",
    "    return recon_loss + kl_div * beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e90ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, dataloader, device, num_epoch):\n",
    "    \"\"\"\n",
    "    Train the VAE model.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for epoch in range(num_epoch):\n",
    "        total_loss = 0\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epoch}\")\n",
    "        for i, (images, _) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            recon, mu, logvar = model(images)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_function(images, recon, mu, logvar)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if (i + 1) % 10 == 0 or (i + 1) == len(dataloader):\n",
    "                print(f\"  Batch {i+1}/{len(dataloader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"==> Epoch [{epoch+1}/{num_epoch}] - Average Loss: {avg_loss:.4f}\")\n",
    "    return model, losses      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a9aa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device, num_samples=10):\n",
    "    \"\"\"\n",
    "    Show real vs reconstructed images.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    real_images, _ = next(iter(dataloader))\n",
    "    samples = real_images[:num_samples].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        recon, _, _ = model(samples)\n",
    "\n",
    "    # Denormalize [-1, 1] -> [0, 1]\n",
    "    generated_images = recon * 0.5 + 0.5\n",
    "    samples = samples * 0.5 + 0.5\n",
    "\n",
    "    real_grid = vutils.make_grid(samples.cpu(), nrow=5, padding=2)\n",
    "    generated_grid = vutils.make_grid(generated_images.cpu(), nrow=5, padding=2)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 6))\n",
    "    axes[0].imshow(real_grid.permute(1, 2, 0))\n",
    "    axes[0].set_title(\"Real Images\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(generated_grid.permute(1, 2, 0))\n",
    "    axes[1].set_title(\"Reconstructed Images\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb6f18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    batch_size = 64\n",
    "    lr = 0.0001\n",
    "    num_epoch = 300\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Training on device: {device}\")\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5] * 3, [0.5] * 3)\n",
    "    ])\n",
    "\n",
    "    # Load dataset\n",
    "    train_data = torchvision.datasets.Flowers102(root=\"data/\", split=\"train\", download=True, transform=transform)\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Initialize model\n",
    "    model = VAE(latent_dim=256, img_channels=3).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Train\n",
    "    trained_model, losses = train_model(model, optimizer, train_dataloader, device, num_epoch)\n",
    "\n",
    "    # Plot loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(losses)+1), losses, color='red')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"VAE Training Loss vs Epoch\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Evaluate\n",
    "    evaluate_model(trained_model, train_dataloader, device, num_samples=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b51820e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
